{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/globalit6679/AI-Project-Monitoring/blob/main/Deep%20Learning%20%26%20Machine%20Learning/CW3/DLCV_VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W6QwnYQjBau6"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7eNozc7DBau7"
      },
      "outputs": [],
      "source": [
        "class VGGBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels,batch_norm=False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        conv2_params = {'kernel_size': (3, 3),\n",
        "                        'stride'     : (1, 1),\n",
        "                        'padding'   : 1\n",
        "                        }\n",
        "\n",
        "        noop = lambda x : x\n",
        "\n",
        "        self._batch_norm = batch_norm\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels,out_channels=out_channels , **conv2_params)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels) if batch_norm else noop\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=out_channels,out_channels=out_channels, **conv2_params)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels) if batch_norm else noop\n",
        "\n",
        "        self.max_pooling = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "\n",
        "    @property\n",
        "    def batch_norm(self):\n",
        "        return self._batch_norm\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.max_pooling(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ojIbYjqTBau8"
      },
      "outputs": [],
      "source": [
        "class VGG16(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, num_classes=10,batch_norm=False):\n",
        "        super(VGG16, self).__init__()\n",
        "\n",
        "        self.in_channels,self.in_width,self.in_height = input_size\n",
        "\n",
        "        self.block_1 = VGGBlock(self.in_channels,64,batch_norm=batch_norm)\n",
        "        self.block_2 = VGGBlock(64, 128,batch_norm=batch_norm)\n",
        "        self.block_3 = VGGBlock(128, 256,batch_norm=batch_norm)\n",
        "        self.block_4 = VGGBlock(256,512,batch_norm=batch_norm)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(p=0.65),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(p=0.65),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def input_size(self):\n",
        "        return self.in_channels,self.in_width,self.in_height\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = self.block_3(x)\n",
        "        x = self.block_4(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oIKLh7DpBau9"
      },
      "outputs": [],
      "source": [
        "data_transform = transforms.Compose([\n",
        "  transforms.Resize(32),\n",
        "  transforms.RandomHorizontalFlip(),\n",
        "  transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "  transforms.Resize(32),\n",
        "  transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olxAkAIMBau9",
        "outputId": "e20ba046-00e5-4d79-caad-080b8756a6c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 58604152.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 45632184.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 3536189.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 15045176.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5778140.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# CIFAR10 dataset\n",
        "c_train_dataset = datasets.CIFAR10(root='./data',\n",
        "                                 train=True,\n",
        "                                 transform=data_transform,\n",
        "                                 download=True)\n",
        "\n",
        "c_test_dataset = datasets.CIFAR10(root='./data',\n",
        "                                train=False,\n",
        "                                transform=test_transform,\n",
        "                                download=True)\n",
        "\n",
        "\n",
        "c_train_loader = DataLoader(dataset=c_train_dataset,\n",
        "                          num_workers=2,\n",
        "                          batch_size=64,\n",
        "                          shuffle=True)\n",
        "\n",
        "c_test_loader = DataLoader(dataset=c_test_dataset,\n",
        "                          num_workers=2,\n",
        "                          batch_size=64,\n",
        "                          shuffle=False)\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data',\n",
        "                                 train=True,\n",
        "                                 transform=data_transform,\n",
        "                                 download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data',\n",
        "                                train=False,\n",
        "                                transform=test_transform,\n",
        "                                download=True)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          num_workers=2,\n",
        "                          batch_size=64,\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         num_workers=2,\n",
        "                         drop_last=True,\n",
        "                         batch_size=64,\n",
        "                         shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "de6wVtyBBau-"
      },
      "outputs": [],
      "source": [
        "c_loaders = {\"train\": c_train_loader,\n",
        "           \"test\": c_test_loader}\n",
        "\n",
        "loaders = {\"train\": train_loader,\n",
        "           \"test\": test_loader}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6crRL40Bau-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd29a2e4-27a9-442f-b1bd-a9e2b083dc2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import logging\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "net = VGG16(input_size=(3, 64, 64), batch_norm=True)\n",
        "net2 = VGG16(input_size=(1, 64, 64), batch_norm=True)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
        "optimizer2 = optim.SGD(net2.parameters(), lr = 0.01)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0, verbose=False, path='checkpoint.pt'):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.verbose = verbose\n",
        "        self.path = path\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, test_loss, model):\n",
        "        score = -test_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(test_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(test_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, test_loss, model):\n",
        "        if self.verbose:\n",
        "            print(f'Saving model')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "\n",
        "\n",
        "def train(net, loaders, optimizer, criterion, tensorboard, epochs=100, dev=device, save_param = False, model_name='vgg16'):\n",
        "    try:\n",
        "        early_stopping = EarlyStopping(patience=5, verbose=True)\n",
        "        net = net.to(dev)\n",
        "        history_loss = {\"train\": [], \"test\": []}\n",
        "        history_accuracy = {\"train\": [], \"test\": []}\n",
        "        writer = SummaryWriter(f\"runs/vgg16_{tensorboard}\")\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        writer.add_text('Timestamp: ',timestamp)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            sum_loss = {\"train\": 0, \"test\": 0}\n",
        "            sum_accuracy = {\"train\": 0, \"test\": 0}\n",
        "            for split in [\"train\", \"test\"]:\n",
        "                if split == \"train\":\n",
        "                    net.train()\n",
        "                else:\n",
        "                    net.eval()\n",
        "                for (input, labels) in loaders[split]:\n",
        "                    input = input.to(dev)\n",
        "                    labels = labels.to(dev)\n",
        "                    optimizer.zero_grad()\n",
        "                    pred = net(input)\n",
        "                    labels = labels.long()\n",
        "                    loss = criterion(pred, labels)\n",
        "                    sum_loss[split] += loss.item()\n",
        "                    if split == \"train\":\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    _,pred_label = torch.max(pred, dim = 1)\n",
        "                    pred_labels = (pred_label == labels).float()\n",
        "\n",
        "                    batch_accuracy = pred_labels.sum().item()/input.size(0)\n",
        "                    sum_accuracy[split] += batch_accuracy\n",
        "            epoch_loss = {split: sum_loss[split]/len(loaders[split]) for split in [\"train\", \"test\"]}\n",
        "            epoch_accuracy = {split: sum_accuracy[split]/len(loaders[split]) for split in [\"train\", \"test\"]}\n",
        "\n",
        "            early_stopping(epoch_loss['test'], net)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "            writer.add_scalar(\"Training Loss\", epoch_loss['train'])\n",
        "            writer.add_scalar(\"Training Accuracy\", epoch_accuracy['train'])  # Log accuracy\n",
        "            writer.add_scalar(\"Test Loss\", epoch_loss['test'])\n",
        "            writer.add_scalar(\"Test Accuracy\", epoch_accuracy['test'])  # Log accuracy\n",
        "\n",
        "            for split in [\"train\", \"test\"]:\n",
        "                history_loss[split].append(epoch_loss[split])\n",
        "                history_accuracy[split].append(epoch_accuracy[split])\n",
        "\n",
        "            print(f\"Epoch {epoch+1}:\",\n",
        "                  f\"Train Loss={epoch_loss['train']:.4f},\",\n",
        "                  f\"Train Accuracy={epoch_accuracy['train']:.4f},\",\n",
        "                  f\"Test Loss={epoch_loss['test']:.4f},\",\n",
        "                  f\"Test Accuracy={epoch_accuracy['test']:.4f},\")\n",
        "    finally:\n",
        "        plt.title(\"Loss\")\n",
        "        for split in [\"train\", \"test\"]:\n",
        "            plt.plot(history_loss[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        plt.title(\"Accuracy\")\n",
        "        for split in [\"train\", \"test\"]:\n",
        "            plt.plot(history_accuracy[split], label=split)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "train(net, c_loaders, optimizer, criterion, dev=device, tensorboard='cifar10') # CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# Visualisation samples (CIFAR10)\n",
        "def imshow(img):\n",
        "    img = img.permute(1, 2, 0).cpu().numpy()\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_samples(net, loader, dev=device):\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        correct_samples = []\n",
        "        incorrect_samples = []\n",
        "\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(dev)\n",
        "            labels = labels.to(dev)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            correct_mask = predicted == labels\n",
        "\n",
        "            correct_samples.extend([(inputs[i], labels[i], predicted[i]) for i in range(len(inputs)) if correct_mask[i]])\n",
        "            incorrect_samples.extend([(inputs[i], labels[i], predicted[i]) for i in range(len(inputs)) if not correct_mask[i]])\n",
        "\n",
        "    print(\"Correctly classified samples:\")\n",
        "    for i in range(min(5, len(correct_samples))):\n",
        "        input, label, predicted = correct_samples[i]\n",
        "        print(f\"True Label: {label.item()}, Predicted Label: {predicted.item()}\")\n",
        "        imshow(input)\n",
        "\n",
        "    print(\"\\nIncorrectly classified samples:\")\n",
        "    for i in range(min(5, len(incorrect_samples))):\n",
        "        input, label, predicted = incorrect_samples[i]\n",
        "        print(f\"True Label: {label}, Predicted Label: {predicted}\")\n",
        "        imshow(input)\n",
        "\n",
        "visualize_samples(net, c_loaders['test'], dev=device)"
      ],
      "metadata": {
        "id": "M3hacGsUAKDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnWYgR7MBau-"
      },
      "outputs": [],
      "source": [
        "train(net2, loaders, optimizer2, criterion, dev=device, tensorboard='mnist') # MNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation samples (MNIST)\n",
        "visualize_samples(net2, loaders['test'], dev=device)"
      ],
      "metadata": {
        "id": "2NFXVk7b_qCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05AWbS7oGJsK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}